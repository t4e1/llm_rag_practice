{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ì§€ëŠ¥í˜• RAG ì›Œí¬í”Œë¡œìš° êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "**LangGraphë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• RAG ì‹œìŠ¤í…œ**\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” LangGraphì˜ í•µì‹¬ ìš”ì†Œì¸ **ìƒíƒœ(State)**, **ë…¸ë“œ(Node)**, **ì—£ì§€(Edge)** ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ RAG Agent ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤:\n",
        "\n",
        "### ì‹œìŠ¤í…œ ì£¼ìš” ê¸°ëŠ¥\n",
        "1. **ì§€ëŠ¥í˜• ë¬¸ì„œ ê²€ìƒ‰**: Vector Storeì—ì„œ ê´€ë ¨ ë¬¸ì„œ ìë™ ê²€ìƒ‰\n",
        "2. **ê²°ê³¼ í’ˆì§ˆ í‰ê°€**: LLMì´ ê²€ìƒ‰ ê²°ê³¼ì˜ ì¶©ë¶„ì„±ì„ ìë™ íŒë‹¨\n",
        "3. **ë™ì  ì›¹ ê²€ìƒ‰**: í•„ìš”ì‹œ ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ë³´ê°•\n",
        "4. **ì‚¬ìš©ì ê°œì… ì§€ì **: ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ì— ëŒ€í•œ ì‚¬ìš©ì  ìŠ¹ì¸\n",
        "5. **ì§€ì‹ë² ì´ìŠ¤ í™•ì¥**: ìŠ¹ì¸ëœ ì •ë³´ì˜ ìë™ í•™ìŠµ ë° ì €ì¥\n",
        "6. **ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€**: ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•œ ìµœì¢… ë‹µë³€ ìƒì„±\n",
        "\n",
        "### ì›Œí¬í”Œë¡œìš° íë¦„ë„\n",
        "```\n",
        "                                                                                       \n",
        " START â†’ ê²€ìƒ‰ â†’ í‰ê°€ â†’ (ë¶ˆì¶©ë¶„) â†’ ì›¹ê²€ìƒ‰ â†’ ì €ì¥ì—¬ë¶€(ì‚¬ìš©ì) â†’ (ì €ì¥) â†’ ë‹µë³€ìƒì„± â†’ END  \n",
        "                 â†“                           â†“                                         \n",
        "               (ì¶©ë¶„)                     (ì €ì¥ì•ˆí•¨)                                   \n",
        "                 â†“                           â†“                                         \n",
        "               ë‹µë³€ìƒì„±                     ë‹µë³€ìƒì„±                                   \n",
        "                 â†“                           â†“                                         \n",
        "                END                         END                                        \n",
        "                                                                                       \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í™˜ê²½ ì„¤ì • ë° ê¸°ë³¸ ëª¨ë“ˆ ë¡œë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY ì„¤ì •: sk-proj-qg...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
        "load_dotenv()\n",
        "\n",
        "# API í‚¤ í™•ì¸\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "print(f\"OPENAI_API_KEY ì„¤ì •: {openai_api_key[:10] if openai_api_key else 'None'}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ìƒíƒœ(State) ì •ì˜\n",
        "\n",
        "ê·¸ë˜í”„ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš° ë™ì•ˆ ë°ì´í„°ë¥¼ ê´€ë¦¬í•  ì¤‘ì•™ ìƒíƒœ ê°ì²´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any\n",
        "\n",
        "# ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©í•  ìƒíƒœ ì •ì˜\n",
        "# ì¼ë‹¨ í•„ìš”í•´ ë³´ì´ëŠ” ìƒíƒœë¥¼ ì „ë¶€ ì…ë ¥í•˜ê³  ì´í›„ ë¦¬íŒ©í† ë§ ì‹œ ì •ë¦¬í•  ê²ƒ\n",
        "class RAGState(TypedDict):\n",
        "    question: str                    # ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸\n",
        "    documents: List[Dict[str, Any]]  # 1ì°¨ Retriever ê²€ìƒ‰ ê²°ê³¼\n",
        "    web_results: List[Dict[str, Any]] # ì›¹ ê²€ìƒ‰ ê²°ê³¼ (í•„ìš”ì‹œ)\n",
        "    is_sufficient: bool              # 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ì˜ ì¶©ë¶„ì„± ì—¬ë¶€\n",
        "    user_approval: str               # ì›¹ ê²°ê³¼ ì¶”ê°€ì— ëŒ€í•œ ì‚¬ìš©ì ìŠ¹ì¸ (\"yes\" or \"no\")\n",
        "    final_answer: str                # LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RAG ê¸°ë°˜ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
        "\n",
        "ê¸°ì¡´ ìˆ˜ì—…ìë£Œì˜ RAG êµ¬ì„± ìš”ì†Œë“¤ì„ í™œìš©í•˜ì—¬ Vector Storeì™€ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************XxwA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±\u001b[39;00m\n\u001b[32m     20\u001b[39m embeddings = OpenAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m vector_store = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 4. Retriever ìƒì„±\u001b[39;00m\n\u001b[32m     24\u001b[39m retriever = vector_store.as_retriever(\n\u001b[32m     25\u001b[39m     search_type=\u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     search_kwargs={\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     }\n\u001b[32m     30\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:837\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    835\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:591\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    590\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:479\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    483\u001b[39m         response = response.model_dump()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************XxwA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "# PDF ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# 1. ë¬¸ì„œ ë¡œë”©\n",
        "loader = PyPDFLoader(\"../data/[AI.GOV_í•´ì™¸ë™í–¥]_2025-1í˜¸.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. í…ìŠ¤íŠ¸ ë¶„í• \n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
        "\n",
        "# 4. Retriever ìƒì„±\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\n",
        "        \"k\": 3,\n",
        "        \"lambda_mult\": 0.8\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LLM ë° ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jtw57\\AppData\\Local\\Temp\\ipykernel_37420\\2974204080.py:8: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  web_search_tool = TavilySearchResults(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# LLM ì„¤ì •\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„¤ì •\n",
        "web_search_tool = TavilySearchResults(\n",
        "    max_results=3,\n",
        "    include_answer=True,\n",
        "    include_raw_content=False,\n",
        "    search_depth=\"advanced\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ë…¸ë“œ(Node) í•¨ìˆ˜ë“¤ êµ¬í˜„\n",
        "\n",
        "ê° ê¸°ëŠ¥ ë‹¨ìœ„ë¥¼ ë…ë¦½ì ì¸ íŒŒì´ì¬ í•¨ìˆ˜ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 ê²€ìƒ‰ ë…¸ë“œ (retrieve_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ˆê¸° ì •ë³´ ê²€ìƒ‰\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "\n",
        "    # Vector Storeì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
        "    documents = retriever.invoke(state['question'])\n",
        "    \n",
        "    # Documentsë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜\n",
        "    doc_list = []\n",
        "    for doc in documents:\n",
        "        doc_list.append({\n",
        "            \"content\": doc.page_content,\n",
        "            \"metadata\": doc.metadata,\n",
        "            \"source\": \"vector_store\"\n",
        "        })\n",
        "    \n",
        "    # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "    return {\"documents\" : doc_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 í‰ê°€ ë…¸ë“œ (evaluate_node)\n",
        "- ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œì§€ LLMì„ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def evaluate_node(state: RAGState) -> RAGState:\n",
        "    \n",
        "    # í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "    # ì—¬ê¸°ì„œëŠ” YES/NO ë¡œë§Œ íŒë‹¨í•˜ê³  ìˆì§€ë§Œ, ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” ìˆ˜ì¹˜ì ìœ¼ë¡œ ë¹„êµí•˜ëŠ”ê²Œ í•„ìš”\n",
        "    # ìœ ì‚¬ë„ 0.8 ì •í™•ë„ 0.8 ì´ìƒ ë“± ìˆ˜ì¹˜ë¥¼ ì‚¬ìš©í•  ê²ƒ\n",
        "    evaluation_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"\n",
        "ë‹¹ì‹ ì€ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "í‰ê°€ ê¸°ì¤€:\n",
        "1. ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ê°€?\n",
        "2. ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ì¶©ë¶„í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?\n",
        "3. ë‹µë³€ì„ ì‘ì„±í•˜ê¸°ì— í•„ìš”í•œ í•µì‹¬ ë‚´ìš©ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?\n",
        "\n",
        "ì‘ë‹µì€ ë°˜ë“œì‹œ 'YES' ë˜ëŠ” 'NO'ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
        "YES: ì¶©ë¶„í•¨, NO: ë¶ˆì¶©ë¶„í•¨\n",
        "        \"\"\"),\n",
        "        (\"user\", \"\"\"\n",
        "ì§ˆë¬¸: {question}\n",
        "\n",
        "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:\n",
        "{documents}\n",
        "\n",
        "ì´ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œê°€ìš”? (YES/NO)\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    # ë¬¸ì„œ ë‚´ìš© ì •ë¦¬\n",
        "    # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©\n",
        "    # - ê° ë¬¸ì„œëŠ” \"ë¬¸ì„œ 1:\", \"ë¬¸ì„œ 2:\" ë“±ì˜ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ êµ¬ë¶„\n",
        "    # ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
        "    doc_contents = \"\"\n",
        "    for i, doc in enumerate(state['documents']):\n",
        "        doc_contents += f\"ë¬¸ì„œ {i+1}: {doc['content']}\\n\\n\"\n",
        "    \n",
        "    # ë§ˆì§€ë§‰ ì¤„ë°”ê¿ˆ ì œê±°\n",
        "    doc_contents = doc_contents.rstrip()\n",
        "    \n",
        "    # LLM í‰ê°€ ì‹¤í–‰\n",
        "    evaluation_chain = evaluation_prompt | llm\n",
        "    response = evaluation_chain.invoke({\n",
        "        \"question\": state['question'],\n",
        "        \"documents\": doc_contents\n",
        "    })\n",
        "    \n",
        "    # ê²°ê³¼ íŒë‹¨\n",
        "    # LLMì´ ì†Œë¬¸ìë¡œ ì‘ë‹µí•  ê°€ëŠ¥ì„±ë„ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬\n",
        "    # LLMì˜ ì‘ë‹µì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ 'YES'ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "\n",
        "    # - ì‘ë‹µì´ 'YES'ì¸ ê²½ìš°: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì¶©ë¶„í•¨ (True)\n",
        "    # - ì‘ë‹µì´ 'NO'ì¸ ê²½ìš°: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë¶ˆì¶©ë¶„í•¨ (False)\n",
        "    is_sufficient = \"YES\" in response.content.upper()\n",
        "    \n",
        "    # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "    return {\"is_sufficient\" : is_sufficient}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 ì›¹ ê²€ìƒ‰ ë…¸ë“œ (web_search_node)\n",
        "- ì •ë³´ ë³´ê°•: evaluate_nodeì—ì„œ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨í–ˆì„ ë•Œ, ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì¶”ê°€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def web_search_node(state: RAGState) -> RAGState:\n",
        "    # ì›¹ ê²€ìƒ‰ ì‹¤í–‰\n",
        "    search_results = web_search_tool.invoke({\"query\": state['question']})\n",
        "    \n",
        "    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜\n",
        "    web_docs = []\n",
        "    for result in search_results:\n",
        "        web_docs.append({\n",
        "            \"content\": result.get(\"content\", \"\"),\n",
        "            \"title\": result.get(\"title\", \"\"),\n",
        "            \"url\": result.get(\"url\", \"\"),\n",
        "            \"source\": \"web_search\"\n",
        "        })\n",
        "    \n",
        "    \n",
        "    return {\"web_results\" : web_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 ì‚¬ìš©ì ìŠ¹ì¸ ë…¸ë“œ (human_approval_node)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Human In The Loop (HITL)\n",
        "- AI ì‹œìŠ¤í…œê³¼ ì‚¬ëŒì˜ í˜‘ì—… êµ¬ì¡°  \n",
        "**ì¸ê³µì§€ëŠ¥(AI)** ëª¨ë¸ì˜ í›ˆë ¨, ê²€ì¦, ìš´ì˜ ê³¼ì •ì— ì‚¬ëŒì´ ê°œì…í•˜ì—¬ ì˜ì‚¬ê²°ì •ì„ ë³´ì™„í•˜ëŠ” ë°©ì‹.\n",
        "\n",
        "**HITL(Human In The Loop)ì˜ í•„ìš”ì„±**\n",
        "- AIì˜ ë¶ˆì™„ì „ì„±  \n",
        "    - ë°ì´í„° í¸í–¥(Bias)ê³¼ ë¶ˆí™•ì‹¤ì„±(Uncertainty) ì¡´ì¬\n",
        "    - AI ë‹¨ë… ì˜ì‚¬ê²°ì •ì˜ ìœ„í—˜ì„±\n",
        "    - ì‚¬ëŒì˜ ê²€ì¦ê³¼ ë³´ì™„ í•„ìš”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def human_approval_node(state: RAGState) -> RAGState:\n",
        "    \n",
        "    # print(\"ğŸ»ì›¹ ê²€ìƒ‰ ê²°ê³¼:\")\n",
        "    # for result in state['web_results']:\n",
        "    #     print(f\"\\nì œëª©: {result['title']}\")\n",
        "    #     content = result['content'][:200] + \"...\" if len(result['content']) > 200 else result['content']\n",
        "    #     print(f\"ë‚´ìš©: {content}\")\n",
        "\n",
        "    # ì‚¬ìš©ì ìŠ¹ì¸ ìš”ì²­\n",
        "    user_input = input(\"\\nì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í• ê¹Œìš”? (y/n): \")\n",
        "    \n",
        "    if user_input.lower() == 'y':\n",
        "        approval = \"yes\"\n",
        "    else:\n",
        "        approval = \"no\"\n",
        "\n",
        "    # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "    return {\"user_approval\" : approval}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 ë¬¸ì„œ ì¶”ê°€ ë…¸ë“œ (add_documents_node)\n",
        "- ì§€ì‹ í™•ì¥: ì‚¬ìš©ìê°€ ìŠ¹ì¸í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector Storeì— ì¶”ê°€í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "def add_documents_node(state: RAGState) -> RAGState:\n",
        "    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜\n",
        "    new_docs = []\n",
        "    for result in state['web_results']:\n",
        "        doc = Document(\n",
        "            page_content=result['content'],\n",
        "            metadata={\n",
        "                'title': result.get('title', ''),\n",
        "                'url': result.get('url', ''),\n",
        "                'source': 'web_search',\n",
        "            }\n",
        "        )\n",
        "        new_docs.append(doc)\n",
        "    \n",
        "    # Vector Storeì— ë¬¸ì„œ ì¶”ê°€\n",
        "    # ì¶”í›„ íŒŒì¼í˜•íƒœë¡œ ì €ì¥ê°€ëŠ¥\n",
        "    global vector_store\n",
        "    vector_store.add_documents(new_docs)\n",
        "    \n",
        "    # ê¸°ì¡´ ë¬¸ì„œì— ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€\n",
        "    combined_docs = state['documents'] + state['web_results']\n",
        "    \n",
        "    \n",
        "    return {\"documents\" : combined_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 ë‹µë³€ ìƒì„± ë…¸ë“œ (generate_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    \n",
        "    # ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸\n",
        "    generation_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"\n",
        "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
        "ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n",
        "\n",
        "ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸:\n",
        "1. ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n",
        "2. êµ¬ì²´ì ì¸ ì‚¬ì‹¤ê³¼ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì„¸ìš”\n",
        "3. ë¬¸ì„œì—ì„œ ì§ì ‘ ì¸ìš©í•  ë•ŒëŠ” ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
        "4. ë§Œì•½ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ê·¸ë ‡ê²Œ ëª…ì‹œí•˜ì„¸ìš”\n",
        "5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”\n",
        "        \"\"\"),\n",
        "        (\"user\", \"\"\"\n",
        "ì§ˆë¬¸: {question}\n",
        "\n",
        "ì°¸ê³  ë¬¸ì„œë“¤:\n",
        "{documents}\n",
        "\n",
        "ìœ„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    \n",
        "    all_docs = state.get('documents', [])\n",
        "    \n",
        "    # ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°\n",
        "    doc_contents = \"\"\n",
        "\n",
        "    for doc in all_docs:\n",
        "        source = doc.get('source', 'unknown')\n",
        "        title = doc.get('title', '')\n",
        "        content = doc['content']\n",
        "        doc_contents += f\"[{source}] {title}\\n{content}\\n\\n\"\n",
        "\n",
        "\n",
        "    # ë‹µë³€ ìƒì„±\n",
        "    generation_chain = generation_prompt | llm\n",
        "\n",
        "    response = generation_chain.invoke({\n",
        "        \"question\": state['question'],\n",
        "        \"documents\": doc_contents\n",
        "    })\n",
        "    \n",
        "    final_answer = response.content\n",
        "    \n",
        "    \n",
        "    # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "    return {\"final_answer\" : final_answer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ì¡°ê±´ë¶€ ë¶„ê¸° í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "ê·¸ë˜í”„ì˜ ë™ì  íë¦„ì„ ì œì–´í•˜ëŠ” ì¡°ê±´ë¶€ ë¶„ê¸° í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decide_to_search_web(state: RAGState) -> str:\n",
        "    \n",
        "    if state['is_sufficient']:\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        return \"web_search\"\n",
        "\n",
        "def decide_to_add_documents(state: RAGState) -> str:\n",
        "    \n",
        "    if state['user_approval'] == \"yes\":\n",
        "        return \"add_documents\"\n",
        "    else:\n",
        "        return \"generate\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ê·¸ë˜í”„ êµ¬ì„± ë° ì—£ì§€ ì—°ê²°\n",
        "\n",
        "ëª¨ë“  ë…¸ë“œë¥¼ ì—°ê²°í•˜ì—¬ ì™„ì „í•œ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 1. ìƒíƒœ ê·¸ë˜í”„ ìƒì„±\n",
        "workflow = StateGraph(RAGState)\n",
        "\n",
        "# 2. ëª¨ë“  ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "workflow.add_node(\"evaluate\", evaluate_node)\n",
        "workflow.add_node(\"web_search\", web_search_node)\n",
        "workflow.add_node(\"human_approval\", human_approval_node)\n",
        "workflow.add_node(\"add_documents\", add_documents_node)\n",
        "workflow.add_node(\"generate\", generate_node)\n",
        "\n",
        "# 3. ì—£ì§€ ì—°ê²°\n",
        "# ì‹œì‘ â†’ ê²€ìƒ‰\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "\n",
        "# ê²€ìƒ‰ â†’ í‰ê°€\n",
        "workflow.add_edge(\"retrieve\", \"evaluate\")\n",
        "\n",
        "# í‰ê°€ â†’ ì¡°ê±´ë¶€ ë¶„ê¸° (ì¶©ë¶„í•˜ë©´ generate, ë¶ˆì¶©ë¶„í•˜ë©´ web_search)\n",
        "workflow.add_conditional_edges(\n",
        "    \"evaluate\",\n",
        "    decide_to_search_web,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"web_search\": \"web_search\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# ì›¹ ê²€ìƒ‰ â†’ ì‚¬ìš©ì ìŠ¹ì¸\n",
        "workflow.add_edge(\"web_search\", \"human_approval\")\n",
        "\n",
        "# ì‚¬ìš©ì ìŠ¹ì¸ â†’ ì¡°ê±´ë¶€ ë¶„ê¸° (ìŠ¹ì¸ì‹œ add_documents, ê±°ë¶€ì‹œ generate)\n",
        "workflow.add_conditional_edges(\n",
        "    \"human_approval\",\n",
        "    decide_to_add_documents,\n",
        "    {\n",
        "        \"add_documents\": \"add_documents\",\n",
        "        \"generate\": \"generate\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# ë¬¸ì„œ ì¶”ê°€ â†’ ë‹µë³€ ìƒì„±\n",
        "workflow.add_edge(\"add_documents\", \"generate\")\n",
        "\n",
        "# ë‹µë³€ ìƒì„± â†’ ì¢…ë£Œ\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# 4. ê·¸ë˜í”„ ì»´íŒŒì¼\n",
        "graph = workflow.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ê·¸ë˜í”„ ì‹œê°í™”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "draw_mermaid_png() í•¨ìˆ˜ì—ì„œ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ mermaid ë¬¸ë²•ìœ¼ë¡œ ë°›ì€ê±¸ ì§ì ‘ ë³€í™˜í•´ì„œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "https://www.mermaidchart.com/play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(graph.get_graph().draw_mermaid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory ê¸°ëŠ¥ì„ ìœ„í•œ import\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# In-Memory Checkpointer ìƒì„±\n",
        "# ë©”ëª¨ë¦¬ì— ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•˜ì—¬ ì„¸ì…˜ ë™ì•ˆ ëŒ€í™” ê¸°ë¡ ìœ ì§€\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph_with_memory = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "user1 = RunnableConfig(\n",
        "    recursion_limit=30,\n",
        "    configurable={\"thread_id\": \"1\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "êµ¬ì¶•í•œ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤ì œë¡œ í…ŒìŠ¤íŠ¸í•´ë´…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### í…ŒìŠ¤íŠ¸ 1: ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ê²€ìƒ‰í•´ì•¼ í•  ê²½ìš°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ˆê¸° ìƒíƒœ ì„¤ì •\n",
        "initial_state = {\n",
        "    \"question\": \"ë¯¸êµ­ì˜ AI ì •ì±… ë™í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
        "    \"documents\": [],\n",
        "    \"web_results\": [],\n",
        "    \"is_sufficient\": False,\n",
        "    \"user_approval\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "print(\"í…ŒìŠ¤íŠ¸ 1 ì‹œì‘: ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ê·¸ë˜í”„ ì‹¤í–‰\n",
        "result = graph_with_memory.invoke(initial_state, config=user1)\n",
        "\n",
        "print(\"ìµœì¢… ë‹µë³€:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"final_answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### í…ŒìŠ¤íŠ¸ 2: ì •ë³´ê°€ ìƒì„±ë˜ì–´ ê²€ìƒ‰ì´ í•„ìš”ì—†ì–´ì§„ ê²½ìš°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ˆê¸° ìƒíƒœ ì„¤ì •\n",
        "initial_state = {\n",
        "    \"question\": \"ë¯¸êµ­ì˜ AI ì •ì±… ë™í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
        "    \"documents\": [],\n",
        "    \"web_results\": [],\n",
        "    \"is_sufficient\": False,\n",
        "    \"user_approval\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "print(\"í…ŒìŠ¤íŠ¸ 2 ì‹œì‘: ì¶”ê°€ì ì¸ ì •ë³´ê²€ìƒ‰ì´ í•„ìš”ì—†ì–´ì§ˆ ê²½ìš°\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ê·¸ë˜í”„ ì‹¤í–‰\n",
        "result = graph_with_memory.invoke(initial_state, config=user1)\n",
        "\n",
        "print(\"ìµœì¢… ë‹µë³€:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"final_answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ë° ë¶„ì„\n",
        "\n",
        "ì‹¤í–‰ëœ ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœì™€ ê³¼ì •ì„ ë¶„ì„í•´ë´…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì²« ë²ˆì§¸ ì„¸ì…˜ì˜ ìƒíƒœ ê¸°ë¡ í™•ì¸\n",
        "print(\"ì„¸ì…˜ 1 ì‹¤í–‰ ê¸°ë¡ ë¶„ì„:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, state_snapshot in enumerate(graph_with_memory.get_state_history(user1)):\n",
        "    print(f\"\\në‹¨ê³„ {i}:\")\n",
        "    print(f\"  ë‹¤ìŒ ë…¸ë“œ: {state_snapshot.next}\")\n",
        "    print(f\"  ì§ˆë¬¸: {state_snapshot.values.get('question', ' ')[:50]}...\")\n",
        "    print(f\"  ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(state_snapshot.values.get('documents', []))}\")\n",
        "    print(f\"  ì¶©ë¶„ì„± í‰ê°€: {state_snapshot.values.get('is_sufficient', ' ')}\")\n",
        "    print(f\"  ì›¹ê²€ìƒ‰ê²°ê³¼ ìˆ˜: {len(state_snapshot.values.get('web_results', []))}\")\n",
        "    print(f\"  ì‚¬ìš©ì ì‘ë‹µ: {state_snapshot.values.get('user_approval', '')}\")\n",
        "    print(f\"  ë‹µë³€ ìƒì„± ì—¬ë¶€: {'ì˜ˆ' if state_snapshot.values.get('final_answer') else 'ì•„ë‹ˆì˜¤'}\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì„ì‹œ ì±—ë´‡ì²˜ëŸ¼ ì‚¬ìš©í•´ë³´ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_rag_chat():\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ìì™€ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì§ˆì˜ì‘ë‹µì„ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    print(\"RAG ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\")\n",
        "    print(\"'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    session_counter = 1\n",
        "    \n",
        "    while True:\n",
        "        # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥\n",
        "        user_question = input(\"\\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "        \n",
        "        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸\n",
        "        if user_question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë']:\n",
        "            print(\"ğŸ‘‹ RAG ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!\")\n",
        "            break\n",
        "        \n",
        "        if not user_question:\n",
        "            print(\"âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "            continue\n",
        "        \n",
        "        # ìƒˆë¡œìš´ ì„¸ì…˜ ì„¤ì •\n",
        "        session_config = RunnableConfig(\n",
        "            recursion_limit=50,\n",
        "            configurable={\"thread_id\": f\"interactive_session_{session_counter}\"}\n",
        "        )\n",
        "        \n",
        "        # ì§ˆë¬¸ ì²˜ë¦¬\n",
        "        question_state = {\n",
        "            \"question\": user_question,\n",
        "            \"documents\": [],\n",
        "            \"web_results\": [],\n",
        "            \"is_sufficient\": False,\n",
        "            \"user_approval\": \"\",\n",
        "            \"final_answer\": \"\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            print(\"\\nì²˜ë¦¬ ì¤‘...\")\n",
        "            result = graph_with_memory.invoke(question_state, config=session_config)\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"ë‹µë³€:\")\n",
        "            print(\"=\" * 60)\n",
        "            print(result[\"final_answer\"])\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
        "        \n",
        "        session_counter += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ì±—ë´‡ ì‹¤í–‰í•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ì¸í„°ë™í‹°ë¸Œ ì±—ë´‡ ì‹¤í–‰\n",
        "# interactive_rag_chat()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agentworld",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

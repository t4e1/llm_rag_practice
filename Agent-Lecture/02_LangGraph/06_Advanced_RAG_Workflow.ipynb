{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 지능형 RAG 워크플로우 구현하기\n",
        "\n",
        "**LangGraph를 활용한 지능형 RAG 시스템**\n",
        "\n",
        "이번 실습에서는 LangGraph의 핵심 요소인 **상태(State)**, **노드(Node)**, **엣지(Edge)** 를 활용하여 다음과 같은 RAG Agent 시스템을 구축해보겠습니다:\n",
        "\n",
        "### 시스템 주요 기능\n",
        "1. **지능형 문서 검색**: Vector Store에서 관련 문서 자동 검색\n",
        "2. **결과 품질 평가**: LLM이 검색 결과의 충분성을 자동 판단\n",
        "3. **동적 웹 검색**: 필요시 실시간 웹 검색으로 정보 보강\n",
        "4. **사용자 개입 지점**: 웹 검색 결과 추가에 대한 사용자  승인\n",
        "5. **지식베이스 확장**: 승인된 정보의 자동 학습 및 저장\n",
        "6. **컨텍스트 기반 답변**: 모든 정보를 종합한 최종 답변 생성\n",
        "\n",
        "### 워크플로우 흐름도\n",
        "```\n",
        "                                                                                       \n",
        " START → 검색 → 평가 → (불충분) → 웹검색 → 저장여부(사용자) → (저장) → 답변생성 → END  \n",
        "                 ↓                           ↓                                         \n",
        "               (충분)                     (저장안함)                                   \n",
        "                 ↓                           ↓                                         \n",
        "               답변생성                     답변생성                                   \n",
        "                 ↓                           ↓                                         \n",
        "                END                         END                                        \n",
        "                                                                                       \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 환경 설정 및 기본 모듈 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY 설정: sk-proj-qg...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env 파일에서 환경변수 로드\n",
        "load_dotenv()\n",
        "\n",
        "# API 키 확인\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "print(f\"OPENAI_API_KEY 설정: {openai_api_key[:10] if openai_api_key else 'None'}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 상태(State) 정의\n",
        "\n",
        "그래프의 전체 워크플로우 동안 데이터를 관리할 중앙 상태 객체를 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any\n",
        "\n",
        "# 워크플로우에서 사용할 상태 정의\n",
        "# 일단 필요해 보이는 상태를 전부 입력하고 이후 리팩토링 시 정리할 것\n",
        "class RAGState(TypedDict):\n",
        "    question: str                    # 사용자의 원본 질문\n",
        "    documents: List[Dict[str, Any]]  # 1차 Retriever 검색 결과\n",
        "    web_results: List[Dict[str, Any]] # 웹 검색 결과 (필요시)\n",
        "    is_sufficient: bool              # 1차 검색 결과의 충분성 여부\n",
        "    user_approval: str               # 웹 결과 추가에 대한 사용자 승인 (\"yes\" or \"no\")\n",
        "    final_answer: str                # LLM이 생성한 최종 답변"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RAG 기반 시스템 구축\n",
        "\n",
        "기존 수업자료의 RAG 구성 요소들을 활용하여 Vector Store와 검색 시스템을 구축합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************XxwA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. 임베딩 및 벡터 저장소 생성\u001b[39;00m\n\u001b[32m     20\u001b[39m embeddings = OpenAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m vector_store = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 4. Retriever 생성\u001b[39;00m\n\u001b[32m     24\u001b[39m retriever = vector_store.as_retriever(\n\u001b[32m     25\u001b[39m     search_type=\u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     search_kwargs={\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     }\n\u001b[32m     30\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:837\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    835\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:591\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    590\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:479\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    483\u001b[39m         response = response.model_dump()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jtw57\\miniforge3\\envs\\agentworld\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************XxwA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "# PDF 문서 로딩 및 처리\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# 1. 문서 로딩\n",
        "loader = PyPDFLoader(\"../data/[AI.GOV_해외동향]_2025-1호.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. 텍스트 분할\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. 임베딩 및 벡터 저장소 생성\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
        "\n",
        "# 4. Retriever 생성\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\n",
        "        \"k\": 3,\n",
        "        \"lambda_mult\": 0.8\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LLM 및 웹 검색 도구 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jtw57\\AppData\\Local\\Temp\\ipykernel_37420\\2974204080.py:8: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  web_search_tool = TavilySearchResults(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# LLM 설정\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# 웹 검색 도구 설정\n",
        "web_search_tool = TavilySearchResults(\n",
        "    max_results=3,\n",
        "    include_answer=True,\n",
        "    include_raw_content=False,\n",
        "    search_depth=\"advanced\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 노드(Node) 함수들 구현\n",
        "\n",
        "각 기능 단위를 독립적인 파이썬 함수로 정의합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 검색 노드 (retrieve_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 초기 정보 검색\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "\n",
        "    # Vector Store에서 관련 문서 검색\n",
        "    documents = retriever.invoke(state['question'])\n",
        "    \n",
        "    # Documents를 딕셔너리 형태로 변환\n",
        "    doc_list = []\n",
        "    for doc in documents:\n",
        "        doc_list.append({\n",
        "            \"content\": doc.page_content,\n",
        "            \"metadata\": doc.metadata,\n",
        "            \"source\": \"vector_store\"\n",
        "        })\n",
        "    \n",
        "    # 상태 업데이트\n",
        "    return {\"documents\" : doc_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 평가 노드 (evaluate_node)\n",
        "- 검색된 문서가 질문에 답변하기에 충분한지 LLM을 통해 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def evaluate_node(state: RAGState) -> RAGState:\n",
        "    \n",
        "    # 평가용 프롬프트 생성\n",
        "    # 여기서는 YES/NO 로만 판단하고 있지만, 실제 사용시에는 수치적으로 비교하는게 필요\n",
        "    # 유사도 0.8 정확도 0.8 이상 등 수치를 사용할 것\n",
        "    evaluation_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"\n",
        "당신은 검색 결과의 품질을 평가하는 전문가입니다.\n",
        "주어진 문서들이 사용자의 질문에 답변하기에 충분한지 판단해주세요.\n",
        "\n",
        "평가 기준:\n",
        "1. 문서들이 질문과 직접적으로 관련이 있는가?\n",
        "2. 질문에 대한 구체적이고 충분한 정보가 포함되어 있는가?\n",
        "3. 답변을 작성하기에 필요한 핵심 내용이 모두 포함되어 있는가?\n",
        "\n",
        "응답은 반드시 'YES' 또는 'NO'로만 답변해주세요.\n",
        "YES: 충분함, NO: 불충분함\n",
        "        \"\"\"),\n",
        "        (\"user\", \"\"\"\n",
        "질문: {question}\n",
        "\n",
        "검색된 문서들:\n",
        "{documents}\n",
        "\n",
        "이 문서들이 질문에 답변하기에 충분한가요? (YES/NO)\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    # 문서 내용 정리\n",
        "    # 검색된 모든 문서의 내용을 하나의 문자열로 결합\n",
        "    # - 각 문서는 \"문서 1:\", \"문서 2:\" 등의 번호를 붙여 구분\n",
        "    # 문서 내용을 하나로 합치기\n",
        "    doc_contents = \"\"\n",
        "    for i, doc in enumerate(state['documents']):\n",
        "        doc_contents += f\"문서 {i+1}: {doc['content']}\\n\\n\"\n",
        "    \n",
        "    # 마지막 줄바꿈 제거\n",
        "    doc_contents = doc_contents.rstrip()\n",
        "    \n",
        "    # LLM 평가 실행\n",
        "    evaluation_chain = evaluation_prompt | llm\n",
        "    response = evaluation_chain.invoke({\n",
        "        \"question\": state['question'],\n",
        "        \"documents\": doc_contents\n",
        "    })\n",
        "    \n",
        "    # 결과 판단\n",
        "    # LLM이 소문자로 응답할 가능성도 있으므로 예외 처리\n",
        "    # LLM의 응답을 대문자로 변환하여 'YES'가 포함되어 있는지 확인\n",
        "\n",
        "    # - 응답이 'YES'인 경우: 검색된 문서가 충분함 (True)\n",
        "    # - 응답이 'NO'인 경우: 검색된 문서가 불충분함 (False)\n",
        "    is_sufficient = \"YES\" in response.content.upper()\n",
        "    \n",
        "    # 상태 업데이트\n",
        "    return {\"is_sufficient\" : is_sufficient}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 웹 검색 노드 (web_search_node)\n",
        "- 정보 보강: evaluate_node에서 결과가 불충분하다고 판단했을 때, 웹 검색을 통해 추가 정보를 수집합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def web_search_node(state: RAGState) -> RAGState:\n",
        "    # 웹 검색 실행\n",
        "    search_results = web_search_tool.invoke({\"query\": state['question']})\n",
        "    \n",
        "    # 검색 결과를 딕셔너리 형태로 변환\n",
        "    web_docs = []\n",
        "    for result in search_results:\n",
        "        web_docs.append({\n",
        "            \"content\": result.get(\"content\", \"\"),\n",
        "            \"title\": result.get(\"title\", \"\"),\n",
        "            \"url\": result.get(\"url\", \"\"),\n",
        "            \"source\": \"web_search\"\n",
        "        })\n",
        "    \n",
        "    \n",
        "    return {\"web_results\" : web_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 사용자 승인 노드 (human_approval_node)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Human In The Loop (HITL)\n",
        "- AI 시스템과 사람의 협업 구조  \n",
        "**인공지능(AI)** 모델의 훈련, 검증, 운영 과정에 사람이 개입하여 의사결정을 보완하는 방식.\n",
        "\n",
        "**HITL(Human In The Loop)의 필요성**\n",
        "- AI의 불완전성  \n",
        "    - 데이터 편향(Bias)과 불확실성(Uncertainty) 존재\n",
        "    - AI 단독 의사결정의 위험성\n",
        "    - 사람의 검증과 보완 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def human_approval_node(state: RAGState) -> RAGState:\n",
        "    \n",
        "    # print(\"🐻웹 검색 결과:\")\n",
        "    # for result in state['web_results']:\n",
        "    #     print(f\"\\n제목: {result['title']}\")\n",
        "    #     content = result['content'][:200] + \"...\" if len(result['content']) > 200 else result['content']\n",
        "    #     print(f\"내용: {content}\")\n",
        "\n",
        "    # 사용자 승인 요청\n",
        "    user_input = input(\"\\n웹 검색 결과를 저장할까요? (y/n): \")\n",
        "    \n",
        "    if user_input.lower() == 'y':\n",
        "        approval = \"yes\"\n",
        "    else:\n",
        "        approval = \"no\"\n",
        "\n",
        "    # 상태 업데이트\n",
        "    return {\"user_approval\" : approval}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 문서 추가 노드 (add_documents_node)\n",
        "- 지식 확장: 사용자가 승인한 웹 검색 결과를 Vector Store에 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "def add_documents_node(state: RAGState) -> RAGState:\n",
        "    # 웹 검색 결과를 Document 객체로 변환\n",
        "    new_docs = []\n",
        "    for result in state['web_results']:\n",
        "        doc = Document(\n",
        "            page_content=result['content'],\n",
        "            metadata={\n",
        "                'title': result.get('title', ''),\n",
        "                'url': result.get('url', ''),\n",
        "                'source': 'web_search',\n",
        "            }\n",
        "        )\n",
        "        new_docs.append(doc)\n",
        "    \n",
        "    # Vector Store에 문서 추가\n",
        "    # 추후 파일형태로 저장가능\n",
        "    global vector_store\n",
        "    vector_store.add_documents(new_docs)\n",
        "    \n",
        "    # 기존 문서에 웹 검색 결과 추가\n",
        "    combined_docs = state['documents'] + state['web_results']\n",
        "    \n",
        "    \n",
        "    return {\"documents\" : combined_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 답변 생성 노드 (generate_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    \n",
        "    # 답변 생성용 프롬프트\n",
        "    generation_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"\n",
        "당신은 전문적인 AI 어시스턴트입니다. \n",
        "주어진 문서들을 바탕으로 사용자의 질문에 대해 정확하고 상세한 답변을 제공해주세요.\n",
        "\n",
        "답변 작성 가이드라인:\n",
        "1. 제공된 문서의 내용만을 사용하여 답변하세요\n",
        "2. 구체적인 사실과 데이터를 포함하세요\n",
        "3. 문서에서 직접 인용할 때는 따옴표를 사용하세요\n",
        "4. 만약 충분한 정보가 없다면 그렇게 명시하세요\n",
        "5. 답변은 한국어로 작성하고, 친근하고 이해하기 쉽게 설명하세요\n",
        "        \"\"\"),\n",
        "        (\"user\", \"\"\"\n",
        "질문: {question}\n",
        "\n",
        "참고 문서들:\n",
        "{documents}\n",
        "\n",
        "위 문서들을 바탕으로 질문에 답변해주세요.\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    \n",
        "    all_docs = state.get('documents', [])\n",
        "    \n",
        "    # 모든 문서의 내용을 하나의 문자열로 합치기\n",
        "    doc_contents = \"\"\n",
        "\n",
        "    for doc in all_docs:\n",
        "        source = doc.get('source', 'unknown')\n",
        "        title = doc.get('title', '')\n",
        "        content = doc['content']\n",
        "        doc_contents += f\"[{source}] {title}\\n{content}\\n\\n\"\n",
        "\n",
        "\n",
        "    # 답변 생성\n",
        "    generation_chain = generation_prompt | llm\n",
        "\n",
        "    response = generation_chain.invoke({\n",
        "        \"question\": state['question'],\n",
        "        \"documents\": doc_contents\n",
        "    })\n",
        "    \n",
        "    final_answer = response.content\n",
        "    \n",
        "    \n",
        "    # 상태 업데이트\n",
        "    return {\"final_answer\" : final_answer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 조건부 분기 함수 구현\n",
        "\n",
        "그래프의 동적 흐름을 제어하는 조건부 분기 함수들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decide_to_search_web(state: RAGState) -> str:\n",
        "    \n",
        "    if state['is_sufficient']:\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        return \"web_search\"\n",
        "\n",
        "def decide_to_add_documents(state: RAGState) -> str:\n",
        "    \n",
        "    if state['user_approval'] == \"yes\":\n",
        "        return \"add_documents\"\n",
        "    else:\n",
        "        return \"generate\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 그래프 구성 및 엣지 연결\n",
        "\n",
        "모든 노드를 연결하여 완전한 RAG 워크플로우를 구성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 1. 상태 그래프 생성\n",
        "workflow = StateGraph(RAGState)\n",
        "\n",
        "# 2. 모든 노드를 그래프에 추가\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "workflow.add_node(\"evaluate\", evaluate_node)\n",
        "workflow.add_node(\"web_search\", web_search_node)\n",
        "workflow.add_node(\"human_approval\", human_approval_node)\n",
        "workflow.add_node(\"add_documents\", add_documents_node)\n",
        "workflow.add_node(\"generate\", generate_node)\n",
        "\n",
        "# 3. 엣지 연결\n",
        "# 시작 → 검색\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "\n",
        "# 검색 → 평가\n",
        "workflow.add_edge(\"retrieve\", \"evaluate\")\n",
        "\n",
        "# 평가 → 조건부 분기 (충분하면 generate, 불충분하면 web_search)\n",
        "workflow.add_conditional_edges(\n",
        "    \"evaluate\",\n",
        "    decide_to_search_web,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"web_search\": \"web_search\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# 웹 검색 → 사용자 승인\n",
        "workflow.add_edge(\"web_search\", \"human_approval\")\n",
        "\n",
        "# 사용자 승인 → 조건부 분기 (승인시 add_documents, 거부시 generate)\n",
        "workflow.add_conditional_edges(\n",
        "    \"human_approval\",\n",
        "    decide_to_add_documents,\n",
        "    {\n",
        "        \"add_documents\": \"add_documents\",\n",
        "        \"generate\": \"generate\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# 문서 추가 → 답변 생성\n",
        "workflow.add_edge(\"add_documents\", \"generate\")\n",
        "\n",
        "# 답변 생성 → 종료\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# 4. 그래프 컴파일\n",
        "graph = workflow.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 그래프 시각화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "draw_mermaid_png() 함수에서 문제가 발생하여 mermaid 문법으로 받은걸 직접 변환해서 확인해보겠습니다.\n",
        "\n",
        "https://www.mermaidchart.com/play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(graph.get_graph().draw_mermaid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory 기능을 위한 import\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# In-Memory Checkpointer 생성\n",
        "# 메모리에 대화 상태를 저장하여 세션 동안 대화 기록 유지\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph_with_memory = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "user1 = RunnableConfig(\n",
        "    recursion_limit=30,\n",
        "    configurable={\"thread_id\": \"1\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 시스템 테스트\n",
        "\n",
        "구축한 RAG 워크플로우를 실제로 테스트해봅니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 테스트 1: 정보가 부족하여 검색해야 할 경우\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 초기 상태 설정\n",
        "initial_state = {\n",
        "    \"question\": \"미국의 AI 정책 동향에 대해 설명해주세요\",\n",
        "    \"documents\": [],\n",
        "    \"web_results\": [],\n",
        "    \"is_sufficient\": False,\n",
        "    \"user_approval\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "print(\"테스트 1 시작: 정보가 부족하여 검색하는 경우\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 그래프 실행\n",
        "result = graph_with_memory.invoke(initial_state, config=user1)\n",
        "\n",
        "print(\"최종 답변:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"final_answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 테스트 2: 정보가 생성되어 검색이 필요없어진 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 초기 상태 설정\n",
        "initial_state = {\n",
        "    \"question\": \"미국의 AI 정책 동향에 대해 설명해주세요\",\n",
        "    \"documents\": [],\n",
        "    \"web_results\": [],\n",
        "    \"is_sufficient\": False,\n",
        "    \"user_approval\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "print(\"테스트 2 시작: 추가적인 정보검색이 필요없어질 경우\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 그래프 실행\n",
        "result = graph_with_memory.invoke(initial_state, config=user1)\n",
        "\n",
        "print(\"최종 답변:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"final_answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 시스템 상태 확인 및 분석\n",
        "\n",
        "실행된 워크플로우의 상태와 과정을 분석해봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 첫 번째 세션의 상태 기록 확인\n",
        "print(\"세션 1 실행 기록 분석:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, state_snapshot in enumerate(graph_with_memory.get_state_history(user1)):\n",
        "    print(f\"\\n단계 {i}:\")\n",
        "    print(f\"  다음 노드: {state_snapshot.next}\")\n",
        "    print(f\"  질문: {state_snapshot.values.get('question', ' ')[:50]}...\")\n",
        "    print(f\"  검색된 문서 수: {len(state_snapshot.values.get('documents', []))}\")\n",
        "    print(f\"  충분성 평가: {state_snapshot.values.get('is_sufficient', ' ')}\")\n",
        "    print(f\"  웹검색결과 수: {len(state_snapshot.values.get('web_results', []))}\")\n",
        "    print(f\"  사용자 응답: {state_snapshot.values.get('user_approval', '')}\")\n",
        "    print(f\"  답변 생성 여부: {'예' if state_snapshot.values.get('final_answer') else '아니오'}\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "임시 챗봇처럼 사용해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_rag_chat():\n",
        "    \"\"\"\n",
        "    사용자와 인터랙티브하게 질의응답을 진행하는 함수\n",
        "    \"\"\"\n",
        "    print(\"RAG 챗봇에 오신 것을 환영합니다!\")\n",
        "    print(\"'quit' 또는 'exit'를 입력하면 종료됩니다.\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    session_counter = 1\n",
        "    \n",
        "    while True:\n",
        "        # 사용자 질문 입력\n",
        "        user_question = input(\"\\n❓ 질문을 입력하세요: \").strip()\n",
        "        \n",
        "        # 종료 조건 확인\n",
        "        if user_question.lower() in ['quit', 'exit', '종료', '끝']:\n",
        "            print(\"👋 RAG 챗봇을 종료합니다. 감사합니다!\")\n",
        "            break\n",
        "        \n",
        "        if not user_question:\n",
        "            print(\"❌ 질문을 입력해주세요.\")\n",
        "            continue\n",
        "        \n",
        "        # 새로운 세션 설정\n",
        "        session_config = RunnableConfig(\n",
        "            recursion_limit=50,\n",
        "            configurable={\"thread_id\": f\"interactive_session_{session_counter}\"}\n",
        "        )\n",
        "        \n",
        "        # 질문 처리\n",
        "        question_state = {\n",
        "            \"question\": user_question,\n",
        "            \"documents\": [],\n",
        "            \"web_results\": [],\n",
        "            \"is_sufficient\": False,\n",
        "            \"user_approval\": \"\",\n",
        "            \"final_answer\": \"\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            print(\"\\n처리 중...\")\n",
        "            result = graph_with_memory.invoke(question_state, config=session_config)\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"답변:\")\n",
        "            print(\"=\" * 60)\n",
        "            print(result[\"final_answer\"])\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"처리 중 오류가 발생했습니다: {e}\")\n",
        "        \n",
        "        session_counter += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 챗봇 실행하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 인터랙티브 챗봇 실행\n",
        "# interactive_rag_chat()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agentworld",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
